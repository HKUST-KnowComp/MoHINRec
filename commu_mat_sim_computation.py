#coding=utf8
'''
    calculate the similarity by commuting matrix baesd on meta-structure
'''

import time
import sys
import cPickle as pickle

import numpy as np
import bottleneck as bn
from scipy.sparse import csr_matrix as csr
from str_util import str2unicode

from hin_operator import HIN
from cal_commuting_mat import *

dir_ = 'data/yelp-200k/'

def reverse_map(m):
    return {v:k for k,v in m.items()}

def load_review_aspects():
    '''
        load all the aspect review relations from the file generated by the aspect_extractor.py
    '''
    aspects_dir = dir_  + 'aspects/lda_v1/'
    topic_nums = [10,]
    rid2aspects = {}
    #review_id rate topic_ids raw text
    tn = 10
    aspect_filename = 'review_topic%s.res' % tn
    type_ = 't%s_aspect' % tn
    lines = open(aspects_dir+aspect_filename,'r').readlines()
    parts = [l.strip().split() for l in lines if not l.startswith('#')]
    for info in parts:
        try:
            rid = str2unicode(info[0])
            topic_str = info[2].strip()
            if not topic_str:
                continue
            #topic_ids = [{'topic%s': % t.split(',')[0], } for t in topic_str.split('|')]
            topic_ids = [t.split(',') for t in topic_str.split('|')]
            id2prob = {int(k):float(v) for k, v in topic_ids}
            rid2aspects[rid] = id2prob
        except Exception as e:
            print e
            print topic_str
    return rid2aspects

def get_adj_matrix(rel_type='', head_type='', tail_type=''):
    '''
        Given head_type and tail_type, return the sparse matrix of all the related instances
    '''
    rel_id= hin.get_rel_id_by_name(rel_type)
    relations = hin.get_triplets_by_rel_id(rel_id)

    head_eids = hin.get_entity_ids_by_type(name=head_type)
    tail_eids = hin.get_entity_ids_by_type(name=tail_type)
    head_inds = {v:k for k, v in enumerate(head_eids)}
    tail_inds = {v:k for k, v in enumerate(tail_eids)}
    ind2hids = {k:v for k, v in enumerate(head_eids)}
    ind2tids = {k:v for k, v in enumerate(tail_eids)}

    data, rows, cols = [],[],[]
    for hid, tid, r in relations:
        rows.append(head_inds[hid])
        cols.append(tail_inds[tid])
        data.append(1)
    adj_mat = csr((data,(rows,cols)),shape=[len(head_eids), len(tail_eids)])
    adj_mat_t = csr((data,(cols, rows)),shape=[len(tail_eids), len(head_eids)])
    print 'finish creating adjacency matrix, size=(%s,%s), type=%s' % (len(head_eids), len(tail_eids), rel_type)
    print 'the transpose of adjacency matrix, size=%s' % (adj_mat_t.shape,)
    return adj_mat, ind2hids, ind2tids, adj_mat_t

def save_triplets(filename, triplets, is_append=False):
    if is_append:
        fw = open(filename, 'a+')
    else:
        fw = open(filename, 'w+')
    fw.write('\n'.join(['%s\t%s\t%s' % (h,t,v) for h,t,v in triplets]))
    fw.close()

def cal_commt_mat(meta_structure_str, relation_str):
    '''
        Given meta_structure_str, generate the commuting matrix
        e.g. 'user-review-business,t10_aspect-review-user'
    '''
    #types = meta_structure_str.split('-')
    #relations = relation_str.split('|')
    relation_strs = ['user_review', 'review_business_p', 'review_aspect_p']
    relation_types = [('user', 'review'), ('review', 'business'), ('review', 't10_aspect')]
    UR, u_inds, _, URT = get_adj_matrix(relation_strs[0], relation_types[0][0], relation_types[0][1])
    RB, r_inds, b_inds, RBT = get_adj_matrix(relation_strs[1], relation_types[1][0], relation_types[1][1])
    RA, _, a_inds, RAT = get_adj_matrix(relation_strs[2], relation_types[2][0], relation_types[2][1])
    import pdb;pdb.set_trace()
    RBR = RB.dot(RBT)
    RAR = RA.dot(RAT)
    print 'start element-wise '
    import pdb;pdb.set_trace()
    start = time.time()
    RSR = RBR.multiply(RAR)
    print 'finish element-wise, cost %s seconds' % (time.time() - start)
    URSR = np.dot(UR, RSR)
    URSRU = np.dot(URSR, UR.T)
    import pdb;pdb.set_trace()

def load_review_aspects_from_file(filename):
    lines = open(filename, 'r').readlines()
    tris = [l.strip().split() for l in lines]
    raw_map = {}
    for r,a,w in tris:
        key = '%s_%s' % (r,a)
        raw_map[key] = float(w)
    return raw_map

def generate_adj_mat(relation, row_map, col_map):
    data, rows, cols = [],[],[]
    for r in relation:
        data.append(1)
        rows.append(row_map[r[0]])
        cols.append(col_map[r[1]])
    adj = csr((data,(rows,cols)),shape=[len(row_map), len(col_map)], dtype=np.int64)
    adj.indptr = adj.indptr.astype(np.int64)
    adj.indices = adj.indices.astype(np.int64)
    adj_t = csr((data,(cols, rows)),shape=[len(col_map), len(row_map)], dtype=np.int64)
    adj_t.indptr = adj_t.indptr.astype(np.int64)
    adj_t.indices = adj_t.indices.astype(np.int64)
    return adj, adj_t

def cal_comm_mat_UBB():
    '''
        calculate the commuting matrix in U-B-*-B style
        in fact, only need to calculate BB
        the whole data
    '''
    uid_filename = dir_ + 'uids.txt'
    lines = open(uid_filename, 'r').readlines()
    uids = [int(l.strip()) for l in lines]
    uid2ind = {v:k for k,v in enumerate(uids)}
    ind2uid = reverse_map(uid2ind)
    print 'run cal_comm_mat for %s uids in %s' % (len(uids), uid_filename)

    pos_bid_filename = dir_ + 'pos_bids.txt'
    lines = open(pos_bid_filename, 'r').readlines()
    bids = [int(l.strip()) for l in lines]
    bid2ind = {v:k for k,v in enumerate(bids)}
    ind2bid = reverse_map(bid2ind)
    print 'run cal_comm_mat for %s pos bids in %s' % (len(bids), pos_bid_filename)

    ubp_filename = dir_ + 'uid_pos_bid.txt'
    ubp = np.loadtxt(ubp_filename, dtype=np.int64)
    adj_ub, adj_ub_t = generate_adj_mat(ubp, uid2ind, bid2ind)

    path_str = 'UBStateB'

    #U-pos-B-Cat-B
    if 'State' in path_str:
        sfilename = dir_ + 'pos_bid_state.txt'
    elif 'Cat' in path_str:
        sfilename = dir_ + 'pos_bid_cat.txt'
    elif 'City' in path_str:
        sfilename = dir_ + 'pos_bid_city.txt'
    elif 'Star' in path_str:
        sfilename = dir_ + 'pos_bid_stars.txt'

    lines = open(sfilename, 'r').readlines()
    parts = [l.strip().split() for l in lines]
    bcs = [(int(b), int(c)) for b,c in parts]
    cat2ind = {v:k for k,v in enumerate(set([c for _, c in bcs]))}
    ind2cat = reverse_map(cat2ind)
    adj_bc, adj_bc_t = generate_adj_mat(bcs, bid2ind, cat2ind)

    t1 = time.time()
    #if path_str in ['BCatB','BCityB', 'BStarB', 'BStateB']:
    comm_res = cal_mat_ubb(path_str, adj_ub, adj_bc, adj_bc_t)

    t2 = time.time()
    print 'cal res of %s cost %2.f seconds' % (path_str, t2 - t1)
    print 'comm_res shape=%s,density=%s' % (comm_res.shape, comm_res.nnz * 1.0/comm_res.shape[0]/comm_res.shape[1])
    batch_save_comm_res(path_str, comm_res, ind2uid, ind2bid, is_sample=False)
    t3 = time.time()
    print 'save res of %s cost %2.f seconds' % (path_str, t3 - t2)

def cal_comm_mat_BB():
    '''
        calculate the commuting matrix in U-B-*-B style
        in fact, only need to calculate B-*-B
        the whole data
    '''
    pos_bid_filename = dir_ + 'pos_bids.txt'
    #bids = hin.get_tail_ids_by_head_ids(uids, 1)#user_business_p, get the positive items
    #bids = list(set(bids))
    lines = open(pos_bid_filename, 'r').readlines()
    bids = [int(l.strip()) for l in lines]
    bid2ind = {v:k for k,v in enumerate(bids)}
    ind2bid = reverse_map(bid2ind)
    print 'run cal_comm_mat for %s pos bids in %s' % (len(bids), pos_bid_filename)

    path_str = 'BStateB'

    #U-pos-B-Cat-B
    if 'State' in path_str:
        sfilename = dir_ + 'pos_bid_state.txt'
    elif 'Cat' in path_str:
        sfilename = dir_ + 'pos_bid_cat.txt'
    elif 'City' in path_str:
        sfilename = dir_ + 'pos_bid_city.txt'
    elif 'Star' in path_str:
        sfilename = dir_ + 'pos_bid_stars.txt'

    lines = open(sfilename, 'r').readlines()
    parts = [l.strip().split() for l in lines]
    bcs = [(int(b), int(c)) for b,c in parts]
    cat2ind = {v:k for k,v in enumerate(set([c for _, c in bcs]))}
    ind2cat = reverse_map(cat2ind)
    adj_bc, adj_bc_t = generate_adj_mat(bcs, bid2ind, cat2ind)

    t1 = time.time()
    #if path_str in ['BCatB','BCityB', 'BStarB', 'BStateB']:
    comm_res = cal_mat_bb(path_str, adj_bc, adj_bc_t)

    t2 = time.time()
    print 'cal res of %s cost %2.f seconds' % (path_str, t2 - t1)
    print 'comm_res shape=%s,density=%s' % (comm_res.shape, comm_res.nnz * 1.0/comm_res.shape[0]/comm_res.shape[1])
    batch_save_comm_res(path_str, comm_res, ind2bid, ind2bid, is_sample=False)
    t3 = time.time()
    print 'save res of %s cost %2.f seconds' % (path_str, t3 - t2)

def cal_comm_mat_samples_UUB():
    '''
        10,000 users per time
        calculate the commuting matrix in U-*-U-B style
    '''
    uid_filename = dir_ + 'samples/uids.txt'
    print 'run cal_comm_mat_samples for 10000 users in ', uid_filename
    lines = open(uid_filename, 'r').readlines()
    uids = [int(l.strip()) for l in lines]
    uid2ind = {v:k for k,v in enumerate(uids)}
    ind2uid = reverse_map(uid2ind)

    bids = hin.get_tail_ids_by_head_ids(uids, 1)#user_business_p, get the positive items
    bids = list(set(bids))
    bid2ind = {v:k for k,v in enumerate(bids)}
    ind2bid = reverse_map(bid2ind)

    path_str = 'UPBCatB'

    if 'UU' in path_str:
        sfilename = dir_ + 'samples/user_social.txt'
        lines = open(sfilename, 'r').readlines()
        socials = [l.strip().split() for l in lines]
        socials = [(int(u1), int(u2)) for u1, u2 in socials]
        adj_uu, adj_uu_t = generate_adj_mat(socials, uid2ind, uid2ind)

    if 'A' in path_str:
        aid_filename = dir_ + 'samples/aids.txt'
        aids = [int(l.strip()) for l in open(aid_filename, 'r').readlines()]
        aid2ind = {v:k for k,v in enumerate(aids)}
        ind2aid  = reverse_map(aid2ind)

    if 'R' in path_str:
        rids = hin.get_tail_ids_by_head_ids(uids, 2)#user_reivew
        rid2ind = {v:k for k,v in enumerate(rids)}
        indrid = reverse_map(rid2ind)

    if 'U' in path_str and 'R' in path_str:
        urs = hin.get_relations_by_head_ids(uids, 2)#2: user_review
        adj_ur, adj_ur_t = generate_adj_mat(urs, uid2ind, rid2ind)

    if 'C' in path_str:
        bcs = hin.get_relations_by_head_ids(bids, 10)#business_category
        adj_bc, adj_bc_t = generate_adj_mat(bcs, bid2ind, cid2ind)

    if 'R' in path_str and 'A' in path_str:
        if 'RAPR' in path_str:
            ras = hin.get_relations_by_head_ids(rids, 6)#6: review_aspect_p 7: review_aspect_n
        elif 'RANR' in path_str:
            ras = hin.get_relations_by_head_ids(rids, 7)#6: review_aspect_p 7: review_aspect_n
        adj_ra, adj_ra_t = generate_adj_mat(ras, rid2ind, aid2ind)

    ubp = hin.get_relations_by_head_ids(uids, 1)#user_business_p
    adj_ub, adj_ub_t = generate_adj_mat(ubp, uid2ind, bid2ind)

    t1 = time.time()
    if path_str == 'UUPB':
        comm_res = cal_mat_uub(path_str, adj_uu, adj_uu_t, adj_ub)
    elif path_str == 'URANRUPB' or path_str == 'URAPRUPB':
        comm_res = cal_mat_urarub(path_str, adj_ur, adj_ur_t, adj_ra, adj_ra_t, adj_ub)
    t2 = time.time()
    print 'cal res of %s cost %2.f seconds' % (path_str, t2 - t1)
    save_comm_res(path_str, comm_res, ind2uid, ind2bid)
    t3 = time.time()
    print 'save res of %s cost %2.f seconds' % (path_str, t3 - t2)

def save_comm_res(path_str, comm_res, ind2row, ind2col, is_sample=True):
    triplets = []
    coo = comm_res.tocoo()
    filename = dir_ + 'commuting_mat/%s.res' % path_str
    if is_sample:
        filename = dir_ + 'samples/commuting_mat/%s.res' % path_str
    for r, c, v in zip(coo.row, coo.col,coo.data):
        triplets.append((ind2row[r], ind2col[c], v))
    save_triplets(filename, triplets)
    print 'save adj matrix in %s, (%s)' % (filename, path_str)

def batch_save_comm_res(path_str, comm_res, ind2row, ind2col, is_sample=True):
    coo = comm_res.tocoo(copy=False)
    filename = dir_ + 'commuting_mat/%s.res' % path_str
    step = 10000000
    N = len(coo.row) / step
    for i in range(N+1):
        start_time = time.time()
        triplets = []
        start = i * step
        end = start + step
        rows = coo.row[start:end]
        cols = coo.col[start:end]
        vs = coo.data[start:end]
        for r, c, v in zip(rows, cols, vs):
            triplets.append((ind2row[r], ind2col[c], v))
        save_triplets(filename, triplets, is_append=True)
        print 'finish saving 10M %s triplets in %s, progress: %s/%s, cost %.2f seconds' % (path_str, filename, (i+1) * step, len(coo.data), time.time() - start_time)
    print 'finish saving all %s in (%s)' % (filename, path_str)

def cal_commu_mat(path_str, adj_ur, adj_ur_t, adj_ra, adj_ra_t, adj_ub):
    print 'meta structure str is ', path_str
    t1 = time.time()
    URA = adj_ur.dot(adj_ra)
    t2 = time.time()
    print 'URA cost ', t2 - t1

    URAR = URA.dot(adj_ra_t)
    t3 = time.time()
    print 'URAR cost ', t3 - t2
    URARU = URAR.dot(adj_ur_t)
    t4 = time.time()
    print 'URARU cost ', t4 - t3
    URARUB = URARU.dot(adj_ub)
    t5 = time.time()
    print 'URARUB cost ', t5 - t4
    import pdb;pdb.set_trace()
    return URARUB

def cal_rar_samples():

    uid_filename = dir_ + 'samples/uids.txt'
    print 'run cal_comm_mat_samples for 10000 users in ', uid_filename
    uids = [int(l.strip()) for l in open(uid_filename, 'r').readlines()]
    uids = uids[:1024]
    uid2ind = {v:k for k,v in enumerate(uids)}
    ind2uid = reverse_map(uid2ind)

    aid_filename = dir_ + 'samples/aids.txt'
    aids = [int(l.strip()) for l in open(aid_filename, 'r').readlines()]
    aid2ind = {v:k for k,v in enumerate(aids)}

    rids = hin.get_tail_ids_by_head_ids(uids, 2)#user_reivew
    rid2ind = {v:k for k,v in enumerate(rids)}
    ind2rid = reverse_map(rid2ind)

    ras = hin.get_relations_by_head_ids(rids, 6)

    #10000 samples don't need weights
    ra_filename = dir_ + 'samples/raw_tri.txt'
    raw_map = load_review_aspects_from_file(ra_filename)#{'rid_aid':w}

    data, rows, cols = [],[],[]
    for rid, aid, rel in ras:
        key = '%s_%s' % (rid,aid)
        data.append(raw_map[key])
        rows.append(rid2ind[rid])
        cols.append(aid2ind[aid])
    adj_ra = csr((data,(rows,cols)),shape=[len(rid2ind), len(aid2ind)])
    adj_ra_t = csr((data,(cols, rows)),shape=[len(aid2ind), len(rid2ind)])

    #import pdb;pdb.set_trace()
    #computing by blocks interations
    t1 = time.time()
    RA = adj_ra.toarray()
    t2 = time.time()
    print 'to dense cost %.2f seconds' % (t2 - t1)
    step, topK = 50, 10
    RAR_csr = cal_rar_block(RA, len(rid2ind), ind2rid, step=step, topK=topK)
    try:
        wfilename = dir_ + 'samples/commuting_mat/rar_sparse_matrix.dat'
        fw = open(wfilename, 'w+')
        pickle.dump(RAR_csr, fw, pickle.HIGHEST_PROTOCOL)

        map_filename = dir_ + 'samples/commuting_mat/rar_ind_map.dat'
        fw = open(map_filename, 'w+')
        map_res = []
        for k,v in ind2rid.items():
            map_res.append('%s\t%s' % (k,v))
        fw.write('\n'.join(map_res))
        fw.close()
    except Exception as e:
        print e

    import pdb;pdb.set_trace()
    RAR_coo = RAR_csr.tocoo()#the number of some rows may be less than topK
    test_rows, test_cols = RAR_coo.row, RAR_coo.col#test_rows, test_cols are global inds
    #directly computing by full matrix dot production
    t1 = time.time()
    rar_dot_res = adj_ra.dot(adj_ra_t).toarray()
    RA_res = np.dot(RA, RA.T)
    print 'RA_res == rar_dot_res: %s, RAR_res size %s' % ((RA_res == rar_dot_res).sum(), RA_res.size)
    t2 = time.time()
    col_inds = bn.argpartsort(-rar_dot_res, topK, axis=1)[:,:topK]
    t3 = time.time()
    print 'to dense cost %.2f seconds, select cost %.2f seconds' % (t2 - t1, t3 - t2)
    #cols = col_inds.reshape(col_inds.size, 1)
    dr,dc = col_inds.shape
    #row_inds = np.tile(np.arange(dr).reshape(dr, 1), dc)
    row_inds = np.indices((dr,dc))[0]
    #topK_res = rar_dot_res[row_inds, col_inds]
    #rows = row_inds.reshape(row_inds.size, 1)
    print 'inds test is ', is_topK_correct(test_rows, test_cols, row_inds, col_inds, rar_dot_res)
    #top_res = rar_dot_res[row_inds, col_inds]
    #if (rows == test_rows).sum() == rows.size:
    #    print 'rows correct!!'
    #if (cols == test_cols).sum() == cols.size:
    #    print 'cols correct!!'
    import pdb;pdb.set_trace()

def is_topK_correct(test_rows, test_cols, row_inds, col_inds, rar_dot_res):
    trows = row_inds.flatten()
    tcols = col_inds.flatten()
    test_pairs = set(zip(test_rows, test_cols))
    true_pairs = set(zip(trows, tcols))

    if not test_pairs.issubset(true_pairs):
        invalid_pairs = [t for t in test_pairs if t not in true_pairs]
        extra_pairs = [t for t in true_pairs if t not in test_pairs]
        print 'test pair is not subset of true pairs, (invalid/extra/test/true pairs): %s/%s/%s/%s ' % (len(invalid_pairs), len(extra_pairs), len(test_pairs), len(true_pairs))
        erows = [r[0] for r in extra_pairs]
        ecols = [r[1] for r in extra_pairs]
        print rar_dot_res[erows,ecols].sum()
        import pdb;pdb.set_trace()
        return False
    resi_pairs = true_pairs - test_pairs
    for r,c in list(resi_pairs):
        if rar_dot_res[r,c]:
            print 'invalid: %s-%s' % (r,c)
            return False
    return True

def cal_rar():

    aspects = hin.get_entitities_by_type('t10_aspect')
    aid2ind = {a[0]:ind for ind, a in enumerate(aspects)}#global ind
    #ind2aid = {v:k for k,v in aid2ind.items()}
    ind2aid = reverse_map(aid2ind)
    aid2ent = {a:e for a,e,_ in aspects}

    reviews = hin.get_entitities_by_type('review')
    rid2ent = {r:e for r, e, _ in reviews}
    rid2ind = {r[0]:ind for ind, r in enumerate(reviews)}#global ind
    ind2rid = reverse_map(rid2ind)

    ent2aspects = load_review_aspects()#{'xxxx':{1:3}}

    ras = hin.get_triplets_by_rel_id(6)
    data, rows, cols = [],[],[]
    for rid, aid, rel_id in ras:
        tid = int(aid2ent[aid].replace('topic',''))#topic1 -- 1
        w = ent2aspects[rid2ent[rid]][tid]
        data.append(w)
        rows.append(rid2ind[rid])
        cols.append(aid2ind[aid])

    adj_ra = csr((data, (rows,cols)),shape=[len(rid2ind), len(aid2ind)])
    adj_ra_t = csr((data, (cols, rows)),shape=[len(aid2ind), len(rid2ind)])
    t1 = time.time()
    RA = adj_ra.toarray()
    t2 = time.time()
    print 'to dense cost %.2f seconds' % (t2 - t1)
    RAR_csr = cal_rar_block(RA, len(rid2ind), ind2rid)
    try:
        wfilename = dir_ + 'commuting_mat/rar_sparse_matrix.dat'
        fw = open(wfilename, 'w+')
        pickle.dump(RAR_csr, fw, pickle.HIGHEST_PROTOCOL)
        map_filename = dir_ + 'commuting_mat/rar_ind_map.dat'
        fw = open(map_filename, 'w+')
        map_res = []
        for k,v in ind2rid.items():
            map_res.append('%s\t%s' % (k,v))
        fw.write('\n'.join(map_res))
        fw.close()
    except Exception as e:
        print e

def cal_rar_block(RA, nR, ind2rid, step=10000, topK=100):
    #step, topK = 10000, 100
    if DEBUG:
        RA = np.random.rand(1005,10)
        ind2rid = {k:k for k in range(1005)}
        nR = 1005
        step, topK = 20, 10
        debug_RR = np.dot(RA, RA.T)
        col_inds = bn.argpartsort(-debug_RR, topK, axis=1)[:,:topK]
        dr,dc = col_inds.shape
        row_inds = np.tile(np.arange(dr).reshape(dr,1), dc)
        debug_res = np.zeros((1005,1005))
        debug_res[row_inds, col_inds] = 1

    step_num = RA.shape[0] / step
    data, rows, cols = [],[],[]
    #import pdb;pdb.set_trace()
    for i in range(step_num+1):
        r = i * step
        rblock = RA[r:r+step]

        b_top100_res = []
        b_top100_inds = []
        tmp_res = {}
        #finish 10000 users
        block_start = time.time()
        for j in range(step_num+1):
            c = j * step
            cblock = RA[c:c+step]
            t3 = time.time()

            dot_res = np.dot(rblock, cblock.T)# dot res: 10000 * 10000

            drc = dot_res.shape[1]
            tmp_topK = topK if topK < drc else drc

            top100_inds = bn.argpartsort(-dot_res, tmp_topK, axis=1)[:,:tmp_topK]#10000 * 100,100 indices of the top K weights, column indices in dot_res
            br, bc = top100_inds.shape
            top100_rows = np.tile(np.arange(br).reshape(br,1), bc)#number of colums = colums of top100 inds, usually =100

            top100_res = dot_res[top100_rows, top100_inds]#only need to preserve top 100 weights for global comparing

            b_top100_res.append(top100_res)

            b_top100_inds.append(top100_inds + c)#preserve the global indices, indices need to add the starting value of every block

        block_end = time.time()
        #print 'finish calculating %s-th block(%s*%s), cost %.2f seconds' % (i+1, step, step, block_end - block_start)
        b_top100_inds = np.concatenate(b_top100_inds, axis=1)
        b_top100_res = np.concatenate(b_top100_res, axis=1)

        top100_inds = bn.argpartsort(-b_top100_res, topK, axis=1)[:,:topK]#10000 * 100,100 indices of the top K weights
        tr, tc = top100_inds.shape
        #it may exists that not all 100 weights are zero, prob is very small, processing later
        top100_rows = np.tile(np.arange(tr).reshape(tr,1), tc)

        #global row and col inds are needed for the constructing the sparse matrix for RAR 
        top100_res = b_top100_res[top100_rows, top100_inds]#10000 * 100, some may equal zero
        b_col_top100_inds = b_top100_inds[top100_rows, top100_inds]#global column inds for top100, then we need to get global row inds

        #the following code is used for gurantee all the weights > 0.0, remove 0 weights, very uncommon
        trows, tcols = np.where(top100_res > 0.0)#return all the rows and cols of top100_res
        global_col_inds = b_col_top100_inds[trows, tcols]#value corresponded to the trows + r
        global_row_inds = trows + i * step
        rows.extend(global_row_inds)
        cols.extend(global_col_inds)

        triplets = []
        save_start = time.time()
        #print 'finish selecting top %s for block %s, cost %.2f seconds' % (topK, i+1, save_start - block_end)
        for r, c in zip(global_row_inds, global_col_inds):
            triplets.append((ind2rid[r], ind2rid[c], 1))
        filename = dir_ + 'commuting_mat/RAR/block_%s.dat' % (i+1)
        save_triplets(filename, triplets)
        save_end = time.time()
        print 'finish processing block %s, res saved in %s, %s triplets, cost detail(total/compute/select/save): %.2f/%.2f/%.2f/%.2f seconds ' % (i+1, filename, len(triplets), save_end - block_start, block_end - block_start, save_start - block_end, save_end - save_start)

    data = np.ones(len(rows))

    t4 = time.time()
    RAR_csr = csr((data, (rows, cols)), shape=[nR, nR])
    t5 = time.time()
    #print '10000 res to sparse matrix(%s) cost %.2f seconds' % (RAR_csr.shape, t5 - t4)
    if DEBUG:
        test = RAR_csr.toarray()
        test_res = (test == debug_res)
        if test_res.sum() == test.size:
            print '!!!block matrix equals directly dot matrix, the res is correct!!!'
        else:
            print 'two matrices are not equal, the res is wrong!!!'
    #RAR = 0.5 * (RAR_csr + RAR_csr.transpose())
    #RAR = RAR.ceil()#0.5 to 1, 1 to 1
    if DEBUG:
        import pdb;pdb.set_trace()
    return RAR_csr

if __name__ == '__main__':
    if len(sys.argv) == 2:
        global hin, DEBUG
        DEBUG = False
        RT = int(sys.argv[1])
        if RT == 0:
            DEBUG = True
            cal_rar_block(None, 0, None)
            sys.exit(0)
        #hin = HIN()
        if RT == 1:
            cal_200k_comm_mat_UBB()
        if RT == 2:
            cal_rar()
        if RT == 3:
            cal_rar_samples()
        if RT == 4:
            cal_comm_mat_BB()
        if RT == 5:
            cal_comm_mat_UBB()
    else:
        print 'intput the args to specify the task:\n0:DEBUG for cal_rar_block\n1:cal_comm_mat_samples\n2:cal_rar\n3:cal_rar_samples\n4:cal_comm_mat_BB\n5:cal_comm_mat_UBB'
        sys.exit(0)

